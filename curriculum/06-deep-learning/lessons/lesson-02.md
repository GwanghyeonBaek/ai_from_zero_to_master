# Lesson 02 — MLP 아키텍처와 활성화 함수

## 핵심 개념
- 퍼셉트론 → 다층 퍼셉트론
- ReLU/Sigmoid/Softmax의 역할
- 비선형성이 필요한 이유

## 실패 패턴
- 죽은 ReLU
- saturation으로 인한 학습 정체

## 실습 연결
- lab-03

## 참고자료
- D2L MLP
- CS231n neural networks
