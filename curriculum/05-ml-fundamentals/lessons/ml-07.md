# ML-07: Classification metrics and trade-offs

## Definition
Core concept: **Classification metrics and trade-offs** is required to build leakage-safe and decision-relevant ML systems.

## Assumptions
- Dataset schema is fixed for the run.
- Evaluation split matches deployment horizon.
- Metrics are selected from business cost context.

## Trade-offs
- Simpler setup is faster but may underfit.
- More complex methods improve fit but increase leakage and overfitting risk.
- Strict reproducibility adds overhead but improves reliability.

## Failure Modes
- Wrong target horizon or future-information leakage.
- Metric mismatch with business cost.
- Non-deterministic runs without fixed seed/version capture.

## Worked Example
Model A has higher ROC-AUC, Model B higher PR-AUC; choose B under severe class imbalance.

## Exit Check
- Can explain concept in 3-5 sentences.
- Can identify one misuse case and one correction.
